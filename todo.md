This project is about LLM as judge. It explores the relationship between LLM's perplexity of an answer and its preference of it.

Specifically, we collect two kinds of data:
1. For each question and two answers, prompt the LLM to decide which answer is better, and record this information.
2. Let the same LLM answer this question, but during generation, we only collect the probability distribution of the next token but assign the actual token with the one in the given answer, and then calculate the log perplexity to be the average log probability of [each token to be generated by LLM equals the given answer's token]. In this way, we get one perplexity metric for each of the answer, and compare them.

Then, we compare the two kinds of data line by line and calculate how correlated the data is.

We start from gpt-4o-mini model. Assume the API key is in .env folder.

The dataset is loaded through:
'''
from datasets import load_dataset
ds = load_dataset("willchow66/mmmlu-intersection-filtered", "ar_xy")
'''

The dataset structure is not determined yet. Print the first few lines to see what it looks like.